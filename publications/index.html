<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Research | Venkatesh Pattabiraman</title> <meta name="author" content="Venkatesh Pattabiraman"> <meta name="description" content="Projects before 2024 are either unpublished projects or thesis"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://notvenky.github.io/publications/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Venkatesh </span>Pattabiraman</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About Me</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Research<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/cv_venkatesh_1pg.pdf">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Research</h1> <p class="post-description">Projects before 2024 are either unpublished projects or thesis</p> </header> <article> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/eflesh.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="eflesh.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="pattabiraman2025eflesh" class="col-sm-8"> <div class="title">eFlesh: Highly customizable Magnetic Touch Sensing using Cut-Cell Microstructures</div> <div class="author"> <em>Venkatesh Pattabiraman</em>, Zizhou Huang, Panozzo. Daniele, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Denis Zorin, Lerrel Pinto, Raunaq Bhirangi' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>under review</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://github.com/notvenky/eFlesh" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener noopener noreferrer" target="_blank">Code</a> <a href="https://e-flesh.com/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener noopener noreferrer" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>If human experience is any guide, operating effectively in unstructured environments—such as homes and offices—requires robots to sense the forces they apply during physical interaction. Yet, the lack of a versatile, accessible, and easily customizable tactile sensor has led to fragmented, sensor-specific solutions in general-purpose robotic manipulation—and in many cases, to force-unaware, sensorless approaches. With eFlesh, we aim to bridge this gap by introducing a magnetic tactile sensor that is low-cost, easy to fabricate, and highly customizable. Building an eFlesh sensor requires only four components: a hobbyist 3D printer, off-the-shelf magnets (costing less than $5), a simple CAD model of the desired shape, and a magnetometer circuit board. The sensor is constructed from tiled, parameterized cut-cell microstructures, which allow for tuning both the sensor’s geometry and its mechanical response. To support broad accessibility, we provide an open-source design tool that converts simple convex OBJ/STL files into 3D-printable STLs ready for fabrication. This modular design framework enables users to create application-specific sensors for robot hands, grippers, quadruped feet, and more, and to easily adjust sensitivity to meet the demands of different tasks. Our sensor characterization experiments demonstrate the precision of eFlesh: contact localization accuracy of 0.5 mm, with force prediction errors of 0.27 N along the z-axis and 0.12 N in the x/y-plane. We also present a learning-based slip detection model that generalizes to unseen objects with 95% accuracy, and visuotactile control policies that improve manipulation performance by 40% over vision-only baselines – achieving 90% success rate for a number of precise tasks like plug insertion and credit card swiping, that require sub-mm accuracy for successful completion. All design files, code, trained models, and the CAD-to-eFlesh STL conversion tool are openly available to promote accessibility and encourage widespread adoption.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/visk.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="visk.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="visk2024" class="col-sm-8"> <div class="title">Learning Precise, Contact-Rich Manipulation through Uncalibrated Tactile Skin</div> <div class="author"> <em>Venkatesh Pattabiraman*</em>, Yifeng Cao, Siddhant Haldar, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Lerrel Pinto, Raunaq Bhirangi*' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Best Paper Award, ViTac ICRA</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://visuoskin.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener noopener noreferrer" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>While visuomotor policy learning has advanced robotic manipulation, precisely executing contact-rich tasks remains challenging due to the limitations of vision in reasoning about physical interactions. To address this, recent work has sought to integrate tactile sensing into policy learning. However, many existing approaches rely on optical tactile sensors that are either restricted to recognition tasks or require complex dimensionality reduction steps for policy learning. In this work, we explore learning policies with magnetic skin sensors, which are inherently low-dimensional, highly sensitive, and inexpensive to integrate with robotic platforms. To leverage these sensors effectively, we present the ViSk framework, a simple approach that uses a transformer-based policy and treats skin sensor data as additional tokens alongside visual information. Evaluated on four complex real-world tasks involving credit card swiping, plug insertion, USB insertion, and bookshelf retrieval, ViSk significantly outperforms both vision-only and optical tactile sensing based policies. Further analysis reveals that combining tactile and visual modalities enhances policy performance and spatial generalization, achieving an average improvement of 27.5% across tasks. Videos and more details can be found on https://visuoskin.github.io.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/ftf.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="ftf.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="adeniji2025ftf" class="col-sm-8"> <div class="title">Feel The Force: Contact-Driven Learning from Humans</div> <div class="author"> Ademi Adeniji*, Zhuoran Chen*, Vincent Liu, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Venkatesh Pattabiraman, Siddhant Haldar, Raunaq Bhirangi, Pieter Abbeel, Lerrel Pinto' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>under review</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://github.com/feel-the-force-ftf/feel-the-force" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener noopener noreferrer" target="_blank">Code</a> <a href="https://feel-the-force-ftf.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener noopener noreferrer" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Robots often struggle with fine-grained force control in contact-rich manipulation tasks. While learning from human demonstrations offers a scalable solution, visual observations alone lack the fidelity needed to capture tactile intent. To bridge this gap, we propose Feel the Force (FTF): a framework that learns force-sensitive manipulation from human tactile demonstrations. FTF uses a low-cost tactile glove to measure contact forces and vision-based hand pose estimation to capture human demonstrations. These are used to train a closed-loop transformer policy that predicts robot end-effector trajectories and desired contact forces. At deployment, a PD controller modulates gripper closure to match the predicted forces, enabling precise and adaptive manipulation. FTF generalizes across diverse force-sensitive tasks, achieving a 77% success rate across five manipulation scenarios, and demonstrates robustness to test-time disturbances—highlighting the benefits of grounding robotic control in human tactile behavior.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/anyskin.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="anyskin.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="anyskin2024" class="col-sm-8"> <div class="title">AnySkin: Plug-and-play Skin Sensing for Robotic Touch</div> <div class="author"> <a href="https://raunaqbhirangi.github.io/" rel="external nofollow noopener noopener noreferrer" target="_blank">Raunaq Bhirangi</a>, <em>Venkatesh Pattabiraman</em>, Enes Erciyes, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Yifeng Cao, Tess Hellebrekers, Lerrel Pinto' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>Best Paper Award, Hardware Intelligence RSS 2025; International Conference on Robotics and Automation (ICRA)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://any-skin.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener noopener noreferrer" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>While tactile sensing is widely accepted as an important and useful sensing modality, its use pales in comparison to other sensory modalities like vision and proprioception. AnySkin addresses the critical challenges that impede the use of tactile sensing – versatility, replaceability, and data reusability. Building on the simple design of ReSkin, and decoupling the sensing electronics from the sensing interface, AnySkin makes integration as straightforward as putting on a phone case and connecting a charger. Furthermore, AnySkin is the first uncalibrated tactile-sensor to report cross-instance generalizability of learned manipulation policies. To summarize, this work makes three key contributions: first, we introduce a streamlined fabrication process and a design tool for creating an adhesive-free, durable and easily replaceable magnetic tactile sensor; second, we characterize slip detection and policy learning with the AnySkin sensor; third, we demonstrate zero-shot generalization of models trained on one instance of AnySkin to new instances, and compare it with popular existing tactile solutions like DIGIT and ReSkin. Videos and more details can be found on https://anon-anyskin.github.io.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/ncap.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="ncap.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ncap2024" class="col-sm-8"> <div class="title">Neural Circuit Architectural Priors for Quadruped Locomotion</div> <div class="author"> <a href="https://www.linkedin.com/in/nikhilxb" rel="external nofollow noopener noopener noreferrer" target="_blank">Nikhil Bhattasali</a>, <em>Venkatesh Pattabiraman</em>, <a href="https://lerrelpinto.com/" rel="external nofollow noopener noopener noreferrer" target="_blank">Lerrel Pinto</a>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Grace Lindsay' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>COSYNE and NAISYS - Published</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ncap-quadruped.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener noopener noreferrer" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Learning-based approaches to quadruped locomotion commonly adopt generic policy architectures like fully connected MLPs. As such architectures contain few inductive biases, it is in practice common to incorporate priors in the form of rewards, training curricula, imitation data, or trajectory generators. In nature, animals are born with highly structured connectivity in their nervous systems shaped by evolution to provide useful architectural priors. For instance, a horse can walk within hours of birth and efficiently improve its ability with practice. In this work, we explore the advantages of a biologically inspired ANN architecture for quadruped locomotion. Our architecture achieves good innate performance and better final performance than MLPs, while using less data and orders of magnitude fewer parameters. Our architecture also exhibits better generalization to task variations, even admitting deployment on a physical robot without standard sim-to-real methods. Our work shows that neural circuits can provide valuable priors for locomotion and encourages future work in neural circuit architectural priors.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/hiss.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="hiss.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hiss2024" class="col-sm-8"> <div class="title">Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling</div> <div class="author"> <a href="https://raunaqbhirangi.github.io/" rel="external nofollow noopener noopener noreferrer" target="_blank">Raunaq Bhirangi</a>, C Wang, <em>Venkatesh Pattabiraman</em>, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Carmel Majidi, Gupta. Abhinav, Tess Hellebrekers, Lerrel Pinto' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>International Conference on Machine Learning (ICML) - Published</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://github.com/raunaqbhirangi/hiss" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener noopener noreferrer" target="_blank">Code</a> <a href="https://hiss-csp.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener noopener noreferrer" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Reasoning from sequences of raw sensory data is a ubiquitous problem across fields ranging from medical devices to robotics. These problems often involve using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to predict sequences of desirable physical quantities (e.g. force, inertial measurements).While classical approaches are powerful for locally-linear prediction problems, they often fall short when using real-world sensors. These sensors are typically non-linear, are affected by extraneous variables (e.g. vibration), and exhibit data-dependent drift. For many problems, the prediction task is exacerbated by small labeled datasets since obtaining ground-truth labels requires expensive equipment. In this work, we present Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction. HiSS stacks structured state-space models on top of each other to create a temporal hierarchy. Across six real-world sensor datasets, from tactile-based state prediction to accelerometer-based inertial measurement, HiSS outperforms prior sequence models such as causal transformers, LSTMs, S4, and Mamba by at least 23% on MSE. Our experiments further indicate that HiSS demonstrates efficient scaling to smaller datasets and is compatible with existing data-filtering techniques.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/unav.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="unav.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="unavai4ce" class="col-sm-8"> <div class="title">UNav: Vision-Based Navigation System for Blind/Low-Vision People</div> <div class="author"> Anbang Yang, <em>Venkatesh Pattabiraman</em>, and <a href="https://engineering.nyu.edu/faculty/chen-feng" rel="external nofollow noopener noopener noreferrer" target="_blank">Chen Feng</a> </div> <div class="periodical"> <em></em> 2022 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Elastica.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Elastica.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="elasticaiisc" class="col-sm-8"> <div class="title">Elastica: A Geometrically Nonlinear Elastic Model Flexible Robotic Arm Resin Printer - for Space Applications</div> <div class="author"> Poornakanta Handral, <em>Venkatesh Pattabiraman</em>, and Ramsharan Rangarajan</div> <div class="periodical"> <em></em> 2022 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Handcollage.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Handcollage.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="handbtp" class="col-sm-8"> <div class="title">Design and Development of an SMA Actuated Humanoid (5- Fingered) Manipulator with Underwater Operatability</div> <div class="author"> <em>Venkatesh Pattabiraman</em>, Saurav Kambil, and I.A. Palani</div> <div class="periodical"> <em></em> 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/ugthesis.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="smaflapper21" class="col-sm-8"> <div class="title">Optimization of a Shape Memory Alloy (SMA) Actuated Marine Robotic Flapper</div> <div class="author"> S Jayachandran, Saurav Kambil, <em>Venkatesh Pattabiraman</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'I.A. Palani' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em></em> 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Venkatesh Pattabiraman. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener noopener noreferrer" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener noopener noreferrer">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>