@article{smaflapper21,
  title={Optimization of a Shape Memory Alloy (SMA) Actuated Marine Robotic Flapper},
  author={Jayachandran, S and Kambil, Saurav and Pattabiraman, Venkatesh and Palani, I.A.},
  year={2021}
}

@article{unavai4ce,
  title={UNav: Vision-Based Navigation System for Blind/Low-Vision People},
  author={Yang, Anbang and Pattabiraman, Venkatesh and Feng, Chen},
  year={2022},
  preview={unav.jpg}
}

@article{elasticaiisc,
  title={Elastica: A Geometrically Nonlinear Elastic Model Flexible Robotic Arm Resin Printer - for Space Applications},
  author={Handral, Poornakanta and Pattabiraman, Venkatesh and Rangarajan, Ramsharan},
  year={2022},
  preview={Elastica.png}
}

@article{handbtp,
  title={Design and Development of an SMA Actuated Humanoid (5- Fingered) Manipulator with Underwater Operatability},
  author={Pattabiraman, Venkatesh and Kambil, Saurav and Palani, I.A.},
  year={2022},
  preview={Handcollage.png},
  pdf={ugthesis.pdf}
}

@article{pattabiraman2025eflesh,
  author={Pattabiraman, Venkatesh and Huang, Zizhou and Panozzo. Daniele and Zorin, Denis and Pinto, Lerrel and Bhirangi, Raunaq},
  journal={under review}, 
  title={eFlesh: Highly customizable Magnetic Touch Sensing using Cut-Cell Microstructures},
  abstract={If human experience is any guide, operating effectively in unstructured environments—such as homes and offices—requires robots to sense the forces they apply during physical interaction. Yet, the lack of a versatile, accessible, and easily customizable tactile sensor has led to fragmented, sensor-specific solutions in general-purpose robotic manipulation—and in many cases, to force-unaware, sensorless approaches. With eFlesh, we aim to bridge this gap by introducing a magnetic tactile sensor that is low-cost, easy to fabricate, and highly customizable. Building an eFlesh sensor requires only four components: a hobbyist 3D printer, off-the-shelf magnets (costing less than $5), a simple CAD model of the desired shape, and a magnetometer circuit board. The sensor is constructed from tiled, parameterized cut-cell microstructures, which allow for tuning both the sensor's geometry and its mechanical response. To support broad accessibility, we provide an open-source design tool that converts simple convex OBJ/STL files into 3D-printable STLs ready for fabrication. This modular design framework enables users to create application-specific sensors for robot hands, grippers, quadruped feet, and more, and to easily adjust sensitivity to meet the demands of different tasks. Our sensor characterization experiments demonstrate the precision of eFlesh: contact localization accuracy of 0.5 mm, with force prediction errors of 0.27 N along the z-axis and 0.12 N in the x/y-plane. We also present a learning-based slip detection model that generalizes to unseen objects with 95% accuracy, and visuotactile control policies that improve manipulation performance by 40% over vision-only baselines -- achieving 90% success rate for a number of precise tasks like plug insertion and credit card swiping, that require sub-mm accuracy for successful completion. All design files, code, trained models, and the CAD-to-eFlesh STL conversion tool are openly available to promote accessibility and encourage widespread adoption.},
  code={https://github.com/notvenky/eFlesh},
  website={https://e-flesh.com/},
  year={2025},
  preview={eflesh.png},
  selected={true}
}

@article{visk2024,
  author={Pattabiraman*, Venkatesh and Cao, Yifeng and Haldar, Siddhant and and Pinto, Lerrel and Bhirangi*, Raunaq},
  journal={International Journal on Robotics Research (IJRR) - manuscript under preparation}, 
  title={Learning Precise, Contact-Rich Manipulation through Uncalibrated Tactile Skin},
  abstract={While visuomotor policy learning has advanced robotic manipulation, precisely executing contact-rich tasks remains challenging due to the limitations of vision in reasoning about physical interactions. To address this, recent work has sought to integrate tactile sensing into policy learning. However, many existing approaches rely on optical tactile sensors that are either restricted to recognition tasks or require complex dimensionality reduction steps for policy learning. In this work, we explore learning policies with magnetic skin sensors, which are inherently low-dimensional, highly sensitive, and inexpensive to integrate with robotic platforms. To leverage these sensors effectively, we present the ViSk framework, a simple approach that uses a transformer-based policy and treats skin sensor data as additional tokens alongside visual information. Evaluated on four complex real-world tasks involving credit card swiping, plug insertion, USB insertion, and bookshelf retrieval, ViSk significantly outperforms both vision-only and optical tactile sensing based policies. Further analysis reveals that combining tactile and visual modalities enhances policy performance and spatial generalization, achieving an average improvement of 27.5% across tasks. Videos and more details can be found on https://visuoskin.github.io.},
  website={https://visuoskin.github.io/},
  year={2025},
  preview={visuoskin.png},
  selected={true}
}

@article{anyskin2024,
  author={Bhirangi, Raunaq and Pattabiraman, Venkatesh and Erciyes, Enes and Cao, Yifeng and Hellebrekers, Tess and Pinto, Lerrel},
  journal={International Conference on Robotics and Automation (ICRA) - Published}, 
  title={AnySkin: Plug-and-play Skin Sensing for Robotic Touch},
  abstract={While tactile sensing is widely accepted as an important and useful sensing modality, its use pales in comparison to other sensory modalities like vision and proprioception. AnySkin addresses the critical challenges that impede the use of tactile sensing -- versatility, replaceability, and data reusability. Building on the simple design of ReSkin, and decoupling the sensing electronics from the sensing interface, AnySkin makes integration as straightforward as putting on a phone case and connecting a charger. Furthermore, AnySkin is the first uncalibrated tactile-sensor to report cross-instance generalizability of learned manipulation policies. To summarize, this work makes three key contributions: first, we introduce a streamlined fabrication process and a design tool for creating an adhesive-free, durable and easily replaceable magnetic tactile sensor; second, we characterize slip detection and policy learning with the AnySkin sensor; third, we demonstrate zero-shot generalization of models trained on one instance of AnySkin to new instances, and compare it with popular existing tactile solutions like DIGIT and ReSkin. Videos and more details can be found on https://anon-anyskin.github.io.},
  website={https://any-skin.github.io/},
  year={2024},
  preview={anyskin.png},
  selected={true}
}

@article{ncap2024,
  author={Bhattasali, Nikhil and Pattabiraman, Venkatesh and Pinto, Lerrel and Lindsay, Grace},
  journal={COSYNE and NAISYS - Published}, 
  title={Neural Circuit Architectural Priors for Quadruped Locomotion},
  abstract={Learning-based approaches to quadruped locomotion commonly adopt generic policy architectures like fully connected MLPs. As such architectures contain few inductive biases, it is in practice common to incorporate priors in the form of rewards, training curricula, imitation data, or trajectory generators. In nature, animals are born with highly structured connectivity in their nervous systems shaped by evolution to provide useful architectural priors. For instance, a horse can walk within hours of birth and efficiently improve its ability with practice. In this work, we explore the advantages of a biologically inspired ANN architecture for quadruped locomotion. Our architecture achieves good innate performance and better final performance than MLPs, while using less data and orders of magnitude fewer parameters. Our architecture also exhibits better generalization to task variations, even admitting deployment on a physical robot without standard sim-to-real methods. Our work shows that neural circuits can provide valuable priors for locomotion and encourages future work in neural circuit architectural priors.},
  website={https://ncap-quadruped.github.io/},
  year={2024},
  preview={splash_ncap.png},
}

@article{hiss2024,
  author={Bhirangi, Raunaq and Wang, C and Pattabiraman, Venkatesh and Majidi, Carmel and Gupta. Abhinav and Hellebrekers, Tess and Pinto, Lerrel},
  journal={International Conference on Machine Learning (ICML) - Published}, 
  title={Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling},
  abstract={Reasoning from sequences of raw sensory data is a ubiquitous problem across fields ranging from medical devices to robotics. These problems often involve using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to predict sequences of desirable physical quantities (e.g. force, inertial measurements).While classical approaches are powerful for locally-linear prediction problems, they often fall short when using real-world sensors. These sensors are typically non-linear, are affected by extraneous variables (e.g. vibration), and exhibit data-dependent drift. For many problems, the prediction task is exacerbated by small labeled datasets since obtaining ground-truth labels requires expensive equipment. In this work, we present Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction. HiSS stacks structured state-space models on top of each other to create a temporal hierarchy. Across six real-world sensor datasets, from tactile-based state prediction to accelerometer-based inertial measurement, HiSS outperforms prior sequence models such as causal transformers, LSTMs, S4, and Mamba by at least 23% on MSE. Our experiments further indicate that HiSS demonstrates efficient scaling to smaller datasets and is compatible with existing data-filtering techniques.},
  code={https://github.com/raunaqbhirangi/hiss},
  website={https://hiss-csp.github.io/},
  year={2024},
  preview={icml2024.jpeg},
}

@article{adeniji2025ftf,
  author={Adeniji*, Ademi and Chen*, Zhuoran and Liu, Vincent and Pattabiraman, Venkatesh and Haldar, Siddhant and Bhirangi, Raunaq and Abbeel, Pieter and Pinto, Lerrel},
  journal={under review}, 
  title={Feel The Force: Contact-Driven Learning from Humans},
  abstract={Robots often struggle with fine-grained force control in contact-rich manipulation tasks. While learning from human demonstrations offers a scalable solution, visual observations alone lack the fidelity needed to capture tactile intent. To bridge this gap, we propose Feel the Force (FTF): a framework that learns force-sensitive manipulation from human tactile demonstrations. FTF uses a low-cost tactile glove to measure contact forces and vision-based hand pose estimation to capture human demonstrations. These are used to train a closed-loop transformer policy that predicts robot end-effector trajectories and desired contact forces. At deployment, a PD controller modulates gripper closure to match the predicted forces, enabling precise and adaptive manipulation. FTF generalizes across diverse force-sensitive tasks, achieving a 77% success rate across five manipulation scenarios, and demonstrates robustness to test-time disturbances—highlighting the benefits of grounding robotic control in human tactile behavior.},
  code={https://github.com/feel-the-force-ftf/feel-the-force},
  website={https://feel-the-force-ftf.github.io/},
  year={2025},
  preview={ftf.png},
}